{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "张量与Numpy的ndarray类似。但深度学习框架⼜⽐Numpy的ndarray多⼀些重要功能：\n",
    "⾸先，GPU很好地⽀持加速计算，⽽NumPy仅⽀持CPU计算；\n",
    "其次，张量类⽀持⾃动微分。这些功能使得张量类更适合深度学习\n",
    "\"\"\"\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似numpy的array\n",
    "torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似numpy.arrange\n",
    "x = torch.arange(12)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定数据类型\n",
    "torch.arange(12, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量的维度\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# 获取shape在第0维的数值\n",
    "print(x.shape[0])\n",
    "print(x.size(0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 维度转换\n",
    "x.reshape(3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果变化成二维矩阵时，只需要指定一个维度，另外一个维度会被自动计算\n",
    "# x.reshape(3,4)等价于x.reshape(-1,4)或x.reshape(3,-1)\n",
    "x.reshape(-1, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 零矩阵\n",
    "torch.zeros((2, 3, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单位矩阵\n",
    "torch.ones((2, 3, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7682,  0.0734,  0.3239, -1.3015],\n        [ 0.9683,  1.0871,  1.9214, -1.0443],\n        [ 1.1669, -0.2660, -1.2642,  1.2277]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标准正态分布矩阵\n",
    "torch.randn(3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  4,  6, 10]) tensor([-1,  0,  2,  6]) tensor([ 2,  4,  8, 16]) tensor([ 1,  4, 16, 64]) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n"
     ]
    }
   ],
   "source": [
    "# 默认 + - * / 或者**运算都是对应位置元素进行计算\n",
    "# **运算符是求幂运算\n",
    "# 数学函数也是单个元素计算\n",
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "print(x + y, x - y, x * y, x ** y, torch.exp(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# 点乘\n",
    "x = torch.arange(1, 3)\n",
    "y = torch.arange(3, 5)\n",
    "print(torch.dot(x, y))  # 点乘\n",
    "print(torch.sum(x * y))  # 等价于dot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 30,  36,  42],\n        [ 66,  81,  96],\n        [102, 126, 150]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵相乘\n",
    "x = torch.arange(1, 10).reshape(3,3)\n",
    "print(x)\n",
    "torch.mm(x,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "张量连结\n",
    "dim=0按行连接\n",
    "dim=1按列连接\n",
    "\"\"\"\n",
    "x = torch.ones((3, 3))\n",
    "y = torch.zeros((3, 3))\n",
    "torch.cat((x, y), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False, False, False],\n        [False, False, False],\n        [False, False, False]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逻辑比较，是对应位置进行运算\n",
    "x == y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor(45)\n",
      "tensor([12, 15, 18])\n",
      "tensor([ 6, 15, 24])\n",
      "tensor([[12, 15, 18]])\n",
      "tensor([[ 6],\n",
      "        [15],\n",
      "        [24]])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 5,  7,  9],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 10).reshape(-1, 3)\n",
    "print(x)\n",
    "print(x.sum())  # sum等聚合函数不指定维度，对全部元素进行相加\n",
    "print(x.sum(axis=0))  # 按行加\n",
    "print(x.sum(axis=1))  # 按列加\n",
    "print(x.sum(axis=0, keepdims=True))  # 保持维度\n",
    "print(x.sum(axis=1, keepdims=True))  # 保持维度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3],\n        [ 5,  7,  9],\n        [12, 15, 18]])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumsum，增量和\n",
    "print(x)\n",
    "x.cumsum(axis=0)  # 1行和2行结果保存到2，2和3结果保存到3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# L2范数运算\n",
    "# sum(sqrt(xi^2))\n",
    "u = torch.tensor([3.0, 4.0])\n",
    "print(torch.norm(u))\n",
    "# L1范数\n",
    "# sum(abs(xi))\n",
    "print(torch.sum(torch.abs(u)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch的广播和numpy一样\n",
    "\"\"\"\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [1, 2],\n        [2, 3]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "对于某个维度只有一维的张量和numpy一样会自动广播进行对齐后计算\n",
    "\"\"\"\n",
    "a + b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1287,  0.3021, -0.6453,  0.6403],\n        [-0.0911, -0.5203,  0.0556,  0.2882],\n        [ 0.0153,  2.4225, -0.1487,  0.2871]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引和切⽚\n",
    "x = torch.randn((3, 4))\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0153,  2.4225, -0.1487,  0.2871])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1表示最后一个元素，对于高维数据来说，就是其最高维的最后一组元素\n",
    "x[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1287,  0.3021, -0.6453,  0.6403],\n        [-0.0911, -0.5203,  0.0556,  0.2882]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片是左闭右开\n",
    "x[0:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 内存节约\n",
    "\"\"\"\n",
    "下面说明了x+x的计算结果，生成了新的内存空间\n",
    "这可能是不可取的，原因有两个：\n",
    "1. ⾸先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在⼀秒内多次\n",
    "更新所有参数。通常情况下，我们希望原地执⾏这些更新；\n",
    "2. 如果我们不原地更新，其他引⽤仍然会指向旧的内存位置，这样我们的某些代码可能会⽆意中引⽤旧\n",
    "的参数\n",
    "\"\"\"\n",
    "before = id(x)\n",
    "x = x + x\n",
    "id(x) == before"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "使用[:]操作进行原地更新\n",
    "\"\"\"\n",
    "before = id(x)\n",
    "x[:] = x + x\n",
    "id(x) == before"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1., -1., -1.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "----------------\n",
      "tensor([[-1., -1., -1., -1.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "----------------\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "复制\n",
    "\"\"\"\n",
    "x = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "y = x\n",
    "z = x.clone()\n",
    "x[0] = -1\n",
    "print(x, y, z, sep=\"\\n----------------\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Tensor, numpy.ndarray)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为numpy对象\n",
    "type(x), type(x.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(-1.0, float)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "转换为python的基本数据，item()只能对单个tensor元素使用\n",
    "\"\"\"\n",
    "x[0, 0].item(), type(x[0, 0].item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
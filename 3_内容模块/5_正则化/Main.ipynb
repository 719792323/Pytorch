{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 生成 y=0.05+sum(wi*xi)+noise数据，noise未标准差为0.01正态分布\n",
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, torch.reshape(y, (-1, 1))\n",
    "\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "train_data = synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = load_array(train_data, batch_size)\n",
    "test_data = synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = load_array(test_data, batch_size, is_train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "def init_params():\n",
    "    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    return [w, b]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义l2范数\n",
    "def l2_penalty(w):\n",
    "    return torch.sum(w.pow(2)) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - torch.reshape(y, y_hat.shape)) ** 2 / 2\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(lambd):\n",
    "    w, b = init_params()\n",
    "    net, loss = lambda X: linreg(X, w, b), squared_loss\n",
    "    num_epochs, lr = 100, 0.003\n",
    "    for each in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            # 增加了L2范数惩罚项，\n",
    "            # ⼴播机制使l2_penalty(w)成为⼀个⻓度为batch_size的向量\n",
    "            l = loss(net(X), y) + lambd * l2_penalty(w)\n",
    "            l.sum().backward()\n",
    "            sgd([w, b], lr, batch_size)\n",
    "    print('w的L2范数是:{}'.format(torch.norm(w).item()))\n",
    "    return [w,b]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的L2范数是:12.796683311462402\n",
      "训练集loss： tensor(4.5104e-06, grad_fn=<SumBackward0>)\n",
      "测试集loss:  tensor(8744.3457, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lambda设置为0时，即不设置惩罚项\n",
    "w,b=train(lambd=0)\n",
    "# 训练集loss\n",
    "print(\"训练集loss：\",torch.sum(squared_loss(linreg(train_iter.dataset.tensors[0],w,b),train_iter.dataset.tensors[1])))\n",
    "# 测试集loss\n",
    "print(\"测试集loss: \",torch.sum(squared_loss(linreg(test_iter.dataset.tensors[0],w,b),test_iter.dataset.tensors[1])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的L2范数是:0.37384530901908875\n",
      "训练集loss： tensor(0.0161, grad_fn=<SumBackward0>)\n",
      "测试集loss:  tensor(8.5794, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 设置lambda为3\n",
    "w,b=train(lambd=3)\n",
    "# 训练集loss\n",
    "print(\"训练集loss：\",torch.sum(squared_loss(linreg(train_iter.dataset.tensors[0],w,b),train_iter.dataset.tensors[1])))\n",
    "# 测试集loss\n",
    "print(\"测试集loss: \",torch.sum(squared_loss(linreg(test_iter.dataset.tensors[0],w,b),test_iter.dataset.tensors[1])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}